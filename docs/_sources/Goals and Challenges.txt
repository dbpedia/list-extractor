Goals and Challenges
====================

The main challenge lies in the *extreme variability* of lists: unfortunately there isn't a real standard and there are multiple formats used along with different meanings depending on the user who wrote the page. Also, the strong dependability from the topic as well as the use of natural language makes it impossibile to find a general rule to extract semantic information without knowing in advance the kind of list and the resource type (at least without using advanced AI techniques, which are beyond the purpose of this project). Apart from the heterogeneity, unfortunately there are several Wikipedia pages with bad or wrong formatting, which is obviously reflected in the impurity of extracted data. I have manually tried to correct some of these pages but clearly it is an isignificant percentage.

I started exploring the domains of Writers and Actors and provided datasets for them along with a quality evaluation, but one of my main goals was to provide scalability so that the extractor could be expanded and reach a greater potential. I have (hopefully) achieved that by making the program pick from mapping_rules.py the relations between which mapping to use depending on the current resource domain, and from a given mapping, the keywords to be matched in section titles in order to form a statement. Anyone can extend the mapping process with a new language, simply inserting a new key as a language prefix and a list of keywords from that language. A new domain can be added in a similar way, but also requires a new custom mapping function inside mapper.py. This mechanism is further explained in :doc:`/Mapping Process`. 